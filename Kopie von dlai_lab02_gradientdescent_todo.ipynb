{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1OdK3bNRzjbTWpOqGYzPBMckFE0LvYckb","timestamp":1695902596696}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Logistic regression and Gradient descent\n","This notebook has been created by Oscar Pina (oscar.pina@upc.edu) for the DLAI course (Fall 2023)."],"metadata":{"id":"jEaEBt65FPJO"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"mJziBoEpCjm0","executionInfo":{"status":"ok","timestamp":1695903226996,"user_tz":-120,"elapsed":5539,"user":{"displayName":"Niklas Moser","userId":"04896395378475680868"}}},"outputs":[],"source":["from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","import torch\n","import torch.nn.functional as F\n","import functorch\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"markdown","source":["## Dataset\n","\n","The dataset we are going to use in this notebook is Breast Cancer Wisconsin (diagnosis) dataset [1]. The dataset features are the statistics of the cell nuclei detected in distinct images, and the target is wether the tumor in the image is benign or malign. The dataset contains the mean, standard error and worst value of the cell nuclei texture and morphological features, specifically: the radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry and fractal dimension.\n","\n","[1] [K. P. Bennett and O. L. Mangasarian: “Robust Linear Programming Discrimination of Two Linearly Inseparable Sets”, Optimization Methods and Software 1, 1992, 23-34]."],"metadata":{"id":"HGcoozpPKsAW"}},{"cell_type":"markdown","source":["### Load and visualize data"],"metadata":{"id":"CUXX62eVXKdP"}},{"cell_type":"markdown","source":["In this work, we are only going to use the \"mean\" of each nuclei feature."],"metadata":{"id":"69_2Trn7X1nH"}},{"cell_type":"code","source":["# load dataset\n","dataset = load_breast_cancer(as_frame=True) # include dataframe for visualization\n","\n","# load dataframe\n","df = dataset['frame']\n","\n","# dataset columns with the mean values\n","mean_columns = [ c for c in dataset['frame'].columns if c.startswith(\"mean\") ]\n","\n","N = df.shape[0]\n","D = len(mean_columns)\n","# dataset statistics\n","print(f\"Number of samples:  {N}\")\n","print(f\"Number of features: {D}\")\n","print(f\"Number of negative (benign) samples: {df[df.target==0].shape[0]}\")\n","print(f\"Number of positive (malign) samples: {df[df.target==1].shape[0]}\")"],"metadata":{"id":"6doBbTpxGkW8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695903243837,"user_tz":-120,"elapsed":612,"user":{"displayName":"Niklas Moser","userId":"04896395378475680868"}},"outputId":"1e4b4130-3f7d-4365-dce3-28519ac8207b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of samples:  569\n","Number of features: 10\n","Number of negative (benign) samples: 212\n","Number of positive (malign) samples: 357\n"]}]},{"cell_type":"code","source":["# visualize data (it may take a while)\n","sns.pairplot(df,\n","             x_vars = df[mean_columns],\n","             y_vars = df[mean_columns],\n","             hue='target')\n","plt.plot()"],"metadata":{"id":"R1Oakl7RZUF-","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1mEVfWzBHCMTWKTASZGJqxb_r246kYaxp"},"executionInfo":{"status":"ok","timestamp":1695903293809,"user_tz":-120,"elapsed":38273,"user":{"displayName":"Niklas Moser","userId":"04896395378475680868"}},"outputId":"76f4bae7-6c8d-4fbf-f5e7-474c7c4a63ba"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["As you may have observed, some of the features are redundant, that is, we can see that one feature could easily be predicted from the other one. Here, it would be interesting to carry on some feature engineering techniques to remove redundant information. However, we will skip this step as it is not the main goal of this lab session."],"metadata":{"id":"VyQ79BjbKYzV"}},{"cell_type":"code","source":["# load features and target into X and Y matrices, respectively\n","X = df[mean_columns].values\n","Y = df['target'].values"],"metadata":{"id":"DaBgD_bNaLv3","executionInfo":{"status":"ok","timestamp":1695903325639,"user_tz":-120,"elapsed":222,"user":{"displayName":"Niklas Moser","userId":"04896395378475680868"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### Split data\n","\n","The quality of a model is determined by its generalization abilities rather than memorization, that is, the performance on unseen data during training, not how well does it memorize the data used for training. Therefore, we must split the data into training, validation and test sets. The purpose of these partitions are:\n","1. Training set: data used for training. We compute the loss function and update the parameters of the model based on the result (ie via gradient descent).\n","2. Validation set: data used to tune the hyperparameters of the model such as learning rate, regularization, etc. We measure the performance on this set and select the best configuration for our problem.\n","3. Test set: data used to assess the generalization of the model.\n","\n","For now, do not worry about it, you will see more about this in future lectures. We will use 60 % for training 20 % for validation and the remaining 20 % for testing."],"metadata":{"id":"gi7wjDE_XYYt"}},{"cell_type":"code","source":["# split dataset\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n","X_train, X_val,  Y_train, Y_val  = train_test_split(X_train, Y_train, test_size=0.2/0.8)\n","\n","N_train, N_val, N_test = X_train.shape[0], X_val.shape[0], X_test.shape[0]\n","print(f\"Number of training samples: {N_train}\")\n","print(f\"Number of validation samples: {N_val}\")\n","print(f\"Number of testing samples: {N_test}\")"],"metadata":{"id":"L1IohhA4MJbK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695903365857,"user_tz":-120,"elapsed":277,"user":{"displayName":"Niklas Moser","userId":"04896395378475680868"}},"outputId":"17b9e938-f78b-404c-a272-1b0d0676b451"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training samples: 341\n","Number of validation samples: 114\n","Number of testing samples: 114\n"]}]},{"cell_type":"code","source":["# convert dataset to PyTorch\n","X_train, Y_train = torch.from_numpy(X_train).float(), torch.from_numpy(Y_train).float().unsqueeze(1)\n","X_val,   Y_val   = torch.from_numpy(X_val).float(),   torch.from_numpy(Y_val).float().unsqueeze(1)\n","X_test,  Y_test  = torch.from_numpy(X_test).float(), torch.from_numpy(Y_test).float().unsqueeze(1)"],"metadata":{"id":"VRbfnv_IabEN","executionInfo":{"status":"ok","timestamp":1695903379250,"user_tz":-120,"elapsed":230,"user":{"displayName":"Niklas Moser","userId":"04896395378475680868"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### Standardize the data"],"metadata":{"id":"zo5PszTB77wA"}},{"cell_type":"markdown","source":["We usually need to standardize the data by substracting the mean and scaling by the standard deviation. This is applied to every sample of the dataset:\n","\n","$$\n"," \\mu = \\frac{1}{N} \\sum_{i=1}^N x_i \\in \\mathbb{R}^d\n","$$\n","\n","$$\n","std = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i-\\mu)^2} \\in \\mathbb{R}^d\n","$$\n","\n","Note that $\\mu, std \\in \\mathbb{R}^d$, that is, they can be multi-dimensional. This is due to the fact that the mean and std is independently extracted for every feature, and the standardization is carried out separately too.\n","\n","Traditionally, $\\mu$ and $std$ are computed on the training data, and then used for the training, validation and test samples."],"metadata":{"id":"MWZxoD367_XB"}},{"cell_type":"code","source":["def standardize(x, mean, std):\n","  \"\"\"Standardizes the given data.\n","\n","  Args:\n","    x: A torch tensor of any shape containing the data to be standardized.\n","    mean: A torch tensor of the same shape as x containing the mean of the data.\n","    std: A torch tensor of the same shape as x containing the standard deviation of the data.\n","\n","  Returns:\n","    A torch tensor of the same shape as x containing the standardized data.\n","  \"\"\"\n","  # TODO : standardize the data by the given mean and std\n","  x_std = x/std - mean;\n","  return x_std"],"metadata":{"id":"zqwpGt0Z8X06","executionInfo":{"status":"ok","timestamp":1695903517514,"user_tz":-120,"elapsed":233,"user":{"displayName":"Niklas Moser","userId":"04896395378475680868"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# TODO : get mean of training data\n","# mean = 1/len(X_train) * sum(X_train);\n","mean = torch.mean(X_train, 0);\n","\n","# TODO : get std of training data\n","import math\n","# std = math.sqrt(1/len(X_train) * sum(X_train-mean))\n","std = torch.std(X_train)\n","\n","# standardization\n","X_train = standardize(X_train, mean, std)\n","X_val   = standardize(X_val,   mean, std)\n","X_test  = standardize(X_test,  mean, std)"],"metadata":{"id":"YVaoxXW59uUr","executionInfo":{"status":"ok","timestamp":1695905330898,"user_tz":-120,"elapsed":241,"user":{"displayName":"Niklas Moser","userId":"04896395378475680868"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["## Logistic regression\n","\n","Our model is a log-linear classifier composed by a linear regression layer $(f_{W,b})$, which performs a weighted sum of the input features, followed by a sigmoid function $(\\sigma)$, which converts the output values to probabilities ranging between 0 and 1 as we are dealing with a classification problem.\n","\n","Therefore, given an input sample $x_i$, we make a prediction $\\hat{y}_i=\\sigma( f_{W,b}(x_i) )$, where...\n","\n","- $P(y=1 | x=x_i) = \\hat{y}_i$\n","- $P(y=0 | x=x_i) = 1-\\hat{y}_i$"],"metadata":{"id":"nNV8thsLTg2Y"}},{"cell_type":"markdown","source":["### Define the model\n","The first step is to define the linear regression layer, implemented as a linear transformation applied to the input data and a bias term added to the result.\n","\n","$$\n"," f_{W,b} : x \\in \\mathbb{R}^d → z = x W^T + b \\in \\mathbb{R}\n","$$"],"metadata":{"id":"hvmZNz1Wp7Ul"}},{"cell_type":"code","source":["def linear_regression(x, w, b):\n","  \"\"\"Computes the linear regression prediction for the given input data and weights.\n","\n","  Args:\n","    x: A torch tensor of shape (n_samples, n_features).\n","    w: A torch tensor of shape (n_features, 1).\n","    b: A torch tensor of shape (1, 1).\n","\n","  Returns:\n","    A torch tensor of shape (n_samples, 1) containing the linear regression\n","    predictions.\n","  \"\"\"\n","  # TODO : define the logistic regression layer\n","  z = x*zip(w) + b\n","  return z"],"metadata":{"id":"DB9CfdeqTu9e","executionInfo":{"status":"ok","timestamp":1695904037447,"user_tz":-120,"elapsed":259,"user":{"displayName":"Niklas Moser","userId":"04896395378475680868"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["Nonetheless, a linear transformation can output values that are outside the range [0, 1], so that they cannot be considered as probabilities. To overcome this issue, the sigmoid function is applied:\n","\n"," $$\n"," \\sigma : z \\in \\mathbb{R} → y \\in [0, 1]\n"," $$"],"metadata":{"id":"opA9hTq5qDK4"}},{"cell_type":"code","source":["def sigmoid(x):\n","  \"\"\"Computes the sigmoid function of the given input data.\n","\n","  Args:\n","    x: A torch tensor of any shape.\n","\n","  Returns:\n","    A torch tensor of the same shape as x containing the sigmoid function values.\n","  \"\"\"\n","  # TODO : define the sigmoid function\n","  y = 1 / (1 + math.exp(-x))\n","  return y"],"metadata":{"id":"3PYaM0ITTyt9","executionInfo":{"status":"ok","timestamp":1695904513878,"user_tz":-120,"elapsed":247,"user":{"displayName":"Niklas Moser","userId":"04896395378475680868"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["As you can see, the output of the sigmoid function is restricted to the interval [0, 1]. Being $\\sigma(0.0) = 0.5$:"],"metadata":{"id":"3K-56tmMqSWk"}},{"cell_type":"code","source":["# visualize the function\n","x_range = torch.arange(-10, 10, step=0.5)\n","\n","# plot\n","plt.figure(figsize=(10,5))\n","plt.plot(x_range, sigmoid(x_range))\n","plt.grid()\n","plt.show()"],"metadata":{"id":"IjPgNZVQqVet","colab":{"base_uri":"https://localhost:8080/","height":367},"executionInfo":{"status":"error","timestamp":1695905342943,"user_tz":-120,"elapsed":229,"user":{"displayName":"Niklas Moser","userId":"04896395378475680868"}},"outputId":"a26c9eb8-d948-43a9-d454-20985c565016"},"execution_count":24,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-eb3aea0cd75f>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-8b6b9955a6d9>\u001b[0m in \u001b[0;36msigmoid\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \"\"\"\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# TODO : define the sigmoid function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x500 with 0 Axes>"]},"metadata":{}}]},{"cell_type":"markdown","source":["Finally, we can implement the composition of these two function to get our logistic regressor:"],"metadata":{"id":"Mrksg0ikqcai"}},{"cell_type":"code","source":["def logistic_regression(x, w, b):\n","  \"\"\"Computes the logistic regression prediction for the given input data and weights.\n","\n","  Args:\n","    x: A torch tensor of shape (n_samples, n_features).\n","    w: A torch tensor of shape (n_features, 1).\n","    b: A torch tensor of shape (1, 1).\n","\n","  Returns:\n","    A torch tensor of shape (n_samples, 1) containing the logistic regression\n","    predictions.\n","  \"\"\"\n","\n","  # TODO : Perform linear regression.\n","  z = linear_regression(x,w,b)\n","\n","  # TODO : Apply the sigmoid function.\n","  y = sigmoid(z)\n","\n","  return y"],"metadata":{"id":"vEVRTGwEWL5U","executionInfo":{"status":"ok","timestamp":1695904481050,"user_tz":-120,"elapsed":264,"user":{"displayName":"Niklas Moser","userId":"04896395378475680868"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["### Loss function\n","\n","To train the model for binary classification, we employ the binary cross entropy loss. This quantity measures the cross-entropy between the predicted probability and the ground truth probability, averaged for all data samples.\n","\n","$$\n","J = \\frac{1}{N} \\sum_{i=1}^{N} J_i \\\\ = -\\frac{1}{N} \\sum_{i=1}^{N} \\left [ y_i · logP(y=1|x=x_i) + (1-y_i) · log P(y=0|x=x_i) \\right ] \\\\  = -\\frac{1}{N} \\sum_{i=1}^{N} \\left [ y_i · log(\\hat{y}_i) + (1-y_i) · log(1- \\hat{y}_i) \\right ]\n","$$"],"metadata":{"id":"QK_ui66kqHGZ"}},{"cell_type":"code","source":["def binary_cross_entropy(y_pred, y_true):\n","  \"\"\"Computes the binary cross entropy loss function for the given predicted and\n","  true output values.\n","\n","  Args:\n","    y_pred: A torch tensor of shape (n_samples, 1) containing the predicted\n","      output values.\n","    y_true: A torch tensor of shape (n_samples, 1) containing the true output\n","      values.\n","\n","  Returns:\n","    A torch tensor of shape (1, 1) containing the binary cross entropy loss\n","    value.\n","  \"\"\"\n","  # TODO : Define the binary cross entropy loss\n","  loss = -1/len(y_pred) * sum(y_true*math.log(y_pred)+(1-y_true)*math.log(1-y_pred))\n","  return loss"],"metadata":{"id":"1PsLVqx_T1q1","executionInfo":{"status":"ok","timestamp":1695904948287,"user_tz":-120,"elapsed":235,"user":{"displayName":"Niklas Moser","userId":"04896395378475680868"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"UKlQrEJzqwT6"}},{"cell_type":"markdown","source":["When the target = 1, the cost function is a decreasing function, such that the closer is $P(y=1|x)$ to 1, the lower the cost. Instead, low values of $P(y=1|x)$ are penalized. The opposite intuition applies when target = 0."],"metadata":{"id":"hzJk1gIQv5-k"}},{"cell_type":"code","source":["# visualize the loss function\n","plt.figure(figsize=(5, 5))\n","\n","y_range = torch.arange(0, 1, step=0.01)\n","\n","plt.plot(y_range, [binary_cross_entropy(y_range[i], torch.tensor(0.)) for i in range(y_range.shape[0])], label=\"target = 0\" )\n","plt.plot(y_range, [binary_cross_entropy(y_range[i], torch.tensor(1.)) for i in range(y_range.shape[0])], label=\"target = 1\" )\n","plt.ylabel(\"Loss function $(J_i)$\")\n","plt.xlabel(\"$\\hat{y} = P(y=1|x)$\")\n","plt.legend()\n","plt.grid()"],"metadata":{"id":"XkDoWyD2qhfj","colab":{"base_uri":"https://localhost:8080/","height":409},"executionInfo":{"status":"error","timestamp":1695905368718,"user_tz":-120,"elapsed":287,"user":{"displayName":"Niklas Moser","userId":"04896395378475680868"}},"outputId":"f1e7ec67-a20c-4a2c-bb83-680b7898d873"},"execution_count":25,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-4cf8b7231588>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_range\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"target = 0\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_range\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"target = 1\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss function $(J_i)$\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-4cf8b7231588>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_range\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"target = 0\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_range\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"target = 1\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss function $(J_i)$\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-c720dccb1ef9>\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(y_pred, y_true)\u001b[0m\n\u001b[1;32m     14\u001b[0m   \"\"\"\n\u001b[1;32m     15\u001b[0m   \u001b[0;31m# TODO : Define the binary cross entropy loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    906\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"len() of a 0-d tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             warnings.warn(\n","\u001b[0;31mTypeError\u001b[0m: len() of a 0-d tensor"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 500x500 with 0 Axes>"]},"metadata":{}}]},{"cell_type":"markdown","source":["The loss is optimized with respect to the parameters of the model via gradient descent (next section). Therefore, it is interesting to visualize the loss landscape according to these parameters. Whereas it is not possible to do it in high dimensional spaces, we can work with a lower dimensional problem to get an intuition.\n","\n","In this example we will only work with the *area* feature of our original dataset, so that each data sample is a single scalar $x_i \\in \\mathbb{R}$. Therefore, our model only have 2 parameters: the weight $(w)$ and the bias $(b)$. Let's plot the loss function for our training set wrt $(w)$ and $(b)$."],"metadata":{"id":"BHtkmPP0q-xw"}},{"cell_type":"code","source":["def loss_mesh(x, y, wrange=(-10, 5), brange=(-10,5)):\n","  ws = torch.linspace(wrange[0], wrange[1], 100)\n","  bs = torch.linspace(brange[0], brange[1], 100)\n","\n","  Jwb = torch.zeros(ws.size(0), bs.size(0))\n","  for i in range(ws.size(0)):\n","    for j in range(bs.size(0)):\n","      y_pred = logistic_regression(x, ws[i].view(1,1), bs[j].view(1,1))\n","      Jwb[i, j] = binary_cross_entropy(y_pred, y)\n","  return Jwb, ws, bs\n","\n","def loss_mesh_plot(J, ws, bs):\n","  ww, bb = torch.meshgrid(ws, bs)\n","  plt.figure()\n","  plt.scatter(ww.reshape(-1), bb.reshape(-1), c=Jwb.reshape(-1))\n","  plt.xlabel(\"weight\")\n","  plt.ylabel(\"bias\")"],"metadata":{"id":"sDty-ojPpZmW","executionInfo":{"status":"ok","timestamp":1695905441180,"user_tz":-120,"elapsed":233,"user":{"displayName":"Niklas Moser","userId":"04896395378475680868"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# clone and normalize the data\n","x = X_train[:,0].clone().view(-1,1)\n","y = Y_train.clone()\n","\n","# plot\n","Jwb, ws, bs = loss_mesh(x, y)\n","loss_mesh_plot(Jwb, ws, bs)\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"g6xM8gn2rBqw","executionInfo":{"status":"error","timestamp":1695905446190,"user_tz":-120,"elapsed":265,"user":{"displayName":"Niklas Moser","userId":"04896395378475680868"}},"outputId":"ef08c8dc-a44a-45ca-a914-cbf91e2fceda"},"execution_count":27,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-639d041879d0>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mJwb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_mesh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mloss_mesh_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJwb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-26-051d9823af7d>\u001b[0m in \u001b[0;36mloss_mesh\u001b[0;34m(x, y, wrange, brange)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m       \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m       \u001b[0mJwb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mJwb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-cb3659c3e20d>\u001b[0m in \u001b[0;36mlogistic_regression\u001b[0;34m(x, w, b)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m# TODO : Perform linear regression.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;31m# TODO : Apply the sigmoid function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-b594d92b6d7f>\u001b[0m in \u001b[0;36mlinear_regression\u001b[0;34m(x, w, b)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \"\"\"\n\u001b[1;32m     13\u001b[0m   \u001b[0;31m# TODO : define the logistic regression layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'Tensor' and 'zip'"]}]},{"cell_type":"markdown","source":["**EXERCISE: Why are there empty regions in the loss surface plot?**"],"metadata":{"id":"zuNRntqosPFd"}},{"cell_type":"markdown","source":["## Gradient descent\n","\n","In linear regression, the optimal values of the weight and biases (denoted as $\\hat{w}$ and $\\hat{b}$) can be obtained with a closed-form solution. However, for logistic regression we cannot generally derive that solution. Instead, we employ an iterative optimization algorithm to get the optimal values: gradient descent.\n","\n","The gradient descent algorithm consists of making an initial guess of the values of ${w}$ and ${b}$ (usually initialized as random) and iteratively:\n","\n"," 1. making the prediction on the training data with the given parameters\n","\n"," 2. evaluating the cost:\n"," $$J(w, b)$$\n","\n"," 3. computing the gradients of the cost wrt $w$ and $b$:\n"," $$\\frac{\\partial J}{\\partial w}, \\frac{\\partial J}{\\partial b} $$\n","\n"," 4. updating the values of $w$ and $b$ in the opposite direction of the gradient:\n","\n"," $$ w ← w - \\alpha \\frac{\\partial J}{\\partial w} $$\n","\n","\n"," $$ b ← b - \\alpha \\frac{\\partial J}{\\partial b} $$\n","\n"," Where $\\alpha$ is the learning rate, which defines the length of the step taken towards the opposite direction of the gradient. Note that $w$ and $b$ can be a matrix and a vector, respectively, and $b$ is also included into the set of weights to simplify the notation. All weights and biases are updated simultanieously."],"metadata":{"id":"rJhnHqHdsQzP"}},{"cell_type":"markdown","source":["### Computing the gradients"],"metadata":{"id":"hsNYEBq6sziq"}},{"cell_type":"markdown","source":["The gradients of the binary cross entropy wrt the weight and bias of our model are well known. It is easy to derive them by leveraging the derivative of the sum over the training samples $\\frac{\\partial J}{\\partial w} = \\frac{1}{N} \\sum_{i=1}^N \\frac{\\partial J_i}{\\partial w}$ and noting that.\n","\n","- $\\frac{\\partial J_i}{\\partial w} = (\\sigma(z_i) - y_i)x_i$\n","\n","\n","- $\\frac{\\partial J_i}{\\partial b} = (\\sigma(z_i) - y_i)$\n","\n","where $z_i = f_{W,b}(x_i)$"],"metadata":{"id":"drFcfGjzsbtp"}},{"cell_type":"code","source":["def compute_gradients(x, y_pred, y_true, w, b):\n","  \"\"\"Computes the gradients of the binary cross entropy loss function with\n","  respect to the weights and biases.\n","\n","  Args:\n","    x: A torch tensor of shape (n_samples, n_features).\n","    y_pred: A torch tensor of shape (n_samples, 1) containing the predicted\n","      output values.\n","    y_true: A torch tensor of shape (n_samples, 1) containing the true output\n","      values.\n","    w: A torch tensor of shape (n_features, 1).\n","    b: A torch tensor of shape (1, 1).\n","\n","  Returns:\n","    A dictionary containing the gradients of the binary cross entropy loss\n","    function with respect to the weights and biases.\n","  \"\"\"\n","  # TODO : Compute gradients for the weights\n","  # dw = ...\n","\n","  # TODO : Compute gradients for the bias\n","  # db = ...\n","\n","  return dw, db"],"metadata":{"id":"eONbtNggTsYa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Gradient descent step\n","\n","Given the gradients, we can update the weight and bias parameters in the appropiate direction.\n","\n","Note, that as we can compute directly the values of the gradients, in this problem we do not need to evaluate the loss function at the current values of $w$ and $b$ (step 2). However, for visualization purposes, we are going to evaluate it and return the loss function value. With the given gradients, we can update the current values of $w$ and $b$ towards a more optimal value."],"metadata":{"id":"0DwLpmhus3Ei"}},{"cell_type":"code","source":["def gradient_descent_step(x, y, w, b, learning_rate=1e-3):\n","  \"\"\"Performs one step of gradient descent on the given logistic regression model.\n","\n","  Args:\n","    x: A torch tensor of shape (n_samples, n_features) containing the input data.\n","    y: A torch tensor of shape (n_samples, 1) containing the output data.\n","    w: A torch tensor of shape (n_features, 1) containing the weights of the logistic regression model.\n","    b: A torch tensor of shape (1, 1) containing the bias of the logistic regression model.\n","    learning_rate: A float scalar representing the learning rate.\n","\n","  Returns:\n","    A tuple of three torch tensors, where the first tensor contains loss, the second tensor contains the updated weights and the third tensor contains the updated bias.\n","  \"\"\"\n","\n","  # TODO : Compute the predicted output values.\n","  # y_pred = ...\n","\n","  # TODO : Compute the loss function.\n","  # loss = ...\n","\n","  # TODO : Compute the gradients of the loss function with respect to the weights and bias.\n","  # dw, db = ...\n","\n","  # TODO : Update the weights.\n","  # w = ...\n","\n","  # TODO : Update the bias\n","  # b = ...\n","\n","  return loss, w, b"],"metadata":{"id":"6_g4wrF9VShN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","The length of the step is determined by the length of the gradient itself, but it is controlled by an hyperparameter, the learning rate $(\\alpha)$, which has to be tuned for every problem, model and dataset. Too large values of $\\alpha$ can avoid the convergence of our model, whereas too small values can take forever to converge."],"metadata":{"id":"SOxDG45_twvL"}},{"cell_type":"markdown","source":["### One dimensional example\n","In order to check that the implementation is working properly and visualization purposes to understand what is going behind the scene, we will run our logistic regression model with only one input feature, that is, $x_i \\in \\mathbb{R}$.\n","\n","Feel free to modify the learning rate, initial values for weight and bias as well as the number of gradient descent steps to see the behavior."],"metadata":{"id":"J6vy135Ynn5t"}},{"cell_type":"code","source":["# hyperparameters\n","learning_rate = 1e-1\n","num_steps     = 1000\n","\n","# weight and bias initialization\n","w = torch.tensor([[4.0]])\n","b = torch.tensor([[-4.0]])\n","\n","# clone the data\n","x = X_train[:,0].clone().view(-1,1)\n","y = Y_train.clone()\n","\n","x_val = X_val[:,0].clone().view(-1,1)\n","y_val = Y_val.clone()\n","\n","# gradient descent\n","train_losses, val_losses, ws, bs = list(), list(), list(), list()\n","for _ in range(num_steps):\n","  loss, w, b = gradient_descent_step(x, y, w, b, learning_rate=learning_rate)\n","  train_losses.append(loss)\n","  val_losses.append( binary_cross_entropy( logistic_regression(x_val, w, b), y_val) )\n","  ws.append(w.item())\n","  bs.append(b.item())\n","\n","# visualization\n","\n","# loss mesh\n","Jwb, wgrid, bgrid = loss_mesh(x, y)\n","loss_mesh_plot(Jwb, wgrid, bgrid)\n","plt.plot(ws, bs, color='red', marker='x')\n","plt.show()"],"metadata":{"id":"vJQA371Teusk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We get both training to see if the moder is actually learning and the validation to evaluate wether the model is generalizing. **However, no gradients are computed on the validation set.** As we perform gradient descent steps, we should see the value of the loss function to decrease. It is common to visualize the value of the loss as function of the gradient step iteration:"],"metadata":{"id":"u3SmGZ_Cuzm-"}},{"cell_type":"code","source":["plt.figure()\n","plt.plot(train_losses, label='train')\n","plt.plot(val_losses,   label='val')\n","plt.ylabel(\"Loss\")\n","plt.xlabel(\"Gradient descent iteration\")\n","plt.legend()\n","plt.show()"],"metadata":{"id":"T3DJEox4fi76"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Multidimensional\n","So far, we have worked with one-dimensional input signals $x_i \\in \\mathbb{R}$ for visualization purposes. However, our original dataset contains multiple dimensions and the extension is straightforward. Although we cannot visualize the loss landscape and the gradient updates, we can check that the loss function is actually decreasing after each gradient descent step."],"metadata":{"id":"oUK70iuHgRYr"}},{"cell_type":"markdown","source":["**Exercise: if our input is 10-dimensional, how many parameters are in total in our model? (weights + bias)**"],"metadata":{"id":"wF-I-XVtoVSf"}},{"cell_type":"code","source":["# clone the data\n","x = X_train.clone()\n","y = Y_train.clone()\n","\n","x_val = X_val.clone()\n","y_val = Y_val.clone()\n","\n","# weight and bias initialization\n","w = torch.randn(x.size(1), 1)\n","b = torch.tensor([[0.0]])\n","\n","# gradient descent\n","train_losses, val_losses = list(), list()\n","for _ in range(10000):\n","  loss, w, b = gradient_descent_step(x, y, w, b, learning_rate=1e-3)\n","  train_losses.append(loss)\n","  val_losses.append( binary_cross_entropy( logistic_regression(x_val, w, b), y_val) )\n","\n","plt.figure()\n","plt.plot(train_losses, label='train')\n","plt.plot(val_losses,   label='val')\n","plt.ylabel(\"Loss\")\n","plt.xlabel(\"Gradient descent iteration\")\n","plt.legend()\n","plt.show()"],"metadata":{"id":"oyIn-HkUgTOG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluation\n","\n","There are distinct metrics to assess the quality or performance of a binary classifier. For instance, accuracy is a well know metric that measures the ratio of correctly classified samples. Here, we introduce a distinct set of metrics: precision, recall and F-score.\n","\n","1. The precision measures the ratio between the correctly predicted positive samples and all samples predicted as positive, therefore:\n","\n","$$ P = \\frac{TP}{TP + FP} $$\n","\n","2. The recall, instead, quantifies how many positive instances have been predicted from the total amount of positive instances in the data:\n","\n","$$ R = \\frac{TP}{TP + FN} $$\n","\n","3. Finally, the F-score is an harmonic mean of the previous two metrics:\n","$$ F = 2 \\frac{P * R}{P + R} $$\n"],"metadata":{"id":"YHdQefXJgJpz"}},{"cell_type":"code","source":["def precision(y_pred, y_true):\n","  \"\"\"Computes the precision of a binary classification model.\n","\n","  Args:\n","    y_pred: A torch tensor of shape (n_samples, 1) containing the predicted output values.\n","    y_true: A torch tensor of shape (n_samples, 1) containing the true output values.\n","\n","  Returns:\n","    A torch tensor of shape (1, 1) containing the precision of the model.\n","  \"\"\"\n","\n","  # TODO : get true positive\n","  # tp = ...\n","\n","  # TODO : get false positive\n","  # fp = ...\n","\n","  return tp / (tp + fp)\n","\n","def recall(y_pred, y_true):\n","  \"\"\"Computes the recall of a binary classification model.\n","\n","  Args:\n","    y_pred: A torch tensor of shape (n_samples, 1) containing the predicted output values.\n","    y_true: A torch tensor of shape (n_samples, 1) containing the true output values.\n","\n","  Returns:\n","    A torch tensor of shape (1, 1) containing the recall of the model.\n","  \"\"\"\n","\n","  # TODO : get true positive\n","  # tp = ...\n","\n","  # TODO : get false negative\n","  # fn = ...\n","\n","  return tp / (tp + fn)\n","\n","def f1_score(y_pred, y_true):\n","  \"\"\"Computes the F1 score of a binary classification model.\n","\n","  Args:\n","    y_pred: A torch tensor of shape (n_samples, 1) containing the predicted output values.\n","    y_true: A torch tensor of shape (n_samples, 1) containing the true output values.\n","\n","  Returns:\n","    A torch tensor of shape (1, 1) containing the F1 score of the model.\n","  \"\"\"\n","\n","  p = precision(y_pred, y_true)\n","  r = recall(y_pred, y_true)\n","\n","  return 2 * (p * r) / (p + r)\n","\n","def compute_metrics(y_pred, y_true):\n","  return precision(y_pred, y_true), \\\n","         recall(y_pred, y_true), \\\n","         f1_score(y_pred, y_true)"],"metadata":{"id":"_u66NJ7m6wCL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we can see how our models performs on both training and validation set after each gradient iteration:"],"metadata":{"id":"yNK5xI_X7X4O"}},{"cell_type":"code","source":["# clone and normalize the data\n","x = X_train.clone()\n","y = Y_train.clone()\n","\n","# weight and bias initialization\n","w = torch.randn(x.size(1), 1)\n","b = torch.tensor([[0.0]])\n","\n","# gradient descent\n","losses, ps, rs, fs = list(), list(), list(), list()\n","for _ in range(10000):\n","  # fit step\n","  loss, w, b = gradient_descent_step(x, y,\n","                                     w, b, learning_rate=1e-3)\n","\n","  # eval\n","  y_pred_val = (logistic_regression(X_val, w, b)>0.5).long()\n","  p, r, f = compute_metrics(y_pred_val, Y_val)\n","\n","  ps.append(p)\n","  rs.append(r)\n","  fs.append(f)\n","\n","plt.figure()\n","plt.plot(ps, label='Precision')\n","plt.plot(rs, label='Recall')\n","plt.plot(fs, label='F1-Score')\n","plt.ylabel(\"Metrics\")\n","plt.xlabel(\"Gradient descent iteration\")\n","plt.legend()\n","plt.grid()\n","plt.show()"],"metadata":{"id":"BL8uXWaR7d83"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Chain rule\n","\n","We have computed the gradients of the loss wrt $w$ and $b$ by developing and simplifying the mathematical expression. However, there is another way to do it: applying the chain rule to every step:\n","\n"," $$\n"," \\frac{\\partial J}{\\partial w} = \\frac{1}{N} \\sum_{i=1}^N \\frac{\\partial J_i}{\\partial w} = \\frac{1}{N} \\sum_{i=1}^N \\frac{\\partial J_i}{\\partial \\sigma(z_i)} \\frac{\\partial \\sigma(z_i)}{\\partial z_i} \\frac{\\partial z_i}{\\partial w}\n"," $$."],"metadata":{"id":"4TBU9JQ9v7KU"}},{"cell_type":"code","source":["def dJi_dS(y_pred, y_true):\n","  \"\"\"Computes the derivative of the binary cross entropy loss function with respect to the sigmoid function.\n","\n","  Args:\n","    y_pred: A torch tensor of shape (n_samples, 1) containing the predicted output values.\n","    y_true: A torch tensor of shape (n_samples, 1) containing the true output values.\n","\n","  Returns:\n","    A torch tensor of shape (n_samples, 1) containing the derivative of the binary cross entropy loss function with respect to the sigmoid function.\n","  \"\"\"\n","  # TODO : Compute derivative of the sample loss wrt the sigmoid probabilities (y_pred)\n","  # dS = ...\n","\n","  return dS\n","\n","def dS_dZ(y_pred):\n","  \"\"\"Computes the derivative of the sigmoid function with respect to its input.\n","\n","  Args:\n","    z: A torch tensor of any shape containing the output to the sigmoid function.\n","\n","  Returns:\n","    A torch tensor of the same shape as x containing the derivative of the sigmoid function with respect to the linear projection.\n","  \"\"\"\n","  # TODO : Compute the derivative of the sigmoid\n","  # dz = ...\n","\n","  return dz\n","\n","def dZ_dW(x):\n","  \"\"\"Computes the derivative of the linear projection with respect to the weights.\n","\n","  Args:\n","    x: A torch tensor of shape (n_samples, n_features) containing the input data.\n","\n","  Returns:\n","    A torch tensor of shape (n_features, 1) containing the derivative of the linear projection with respect to the weights.\n","  \"\"\"\n","  # TODO : Compute the derivative of the linear projection wrt W\n","  # dw = ...\n","\n","  return dw\n","\n","def dZ_dB(z):\n","  \"\"\"Computes the derivative of the linear projection with respect to the bias.\n","\n","  Args:\n","    x: A torch tensor of shape (n_samples, n_features) containing the input data.\n","\n","  Returns:\n","    A torch tensor of shape (n_features, 1) containing the derivative of the linear projection with respect to the bias.\n","  \"\"\"\n","  # TODO : Compute the derivative of the linear projection wrt b\n","  # db = ...\n","\n","  return db\n","\n","def compute_gradients_v2(x, z, y_pred, y_true, w, b):\n","  dS  = dJi_dS(y_pred, y_true)\n","  dZ  = dS_dZ(y_pred)\n","  dW  = dZ_dW(x)\n","  dB  = dZ_dB(x)\n","\n","  dw = torch.mean(dW * dZ * dS, dim=0)\n","  db = torch.mean(dB * dZ * dS, dim=0)\n","\n","  return dw, db"],"metadata":{"id":"KF1Gzg5oxgMY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# weight and bias initialization\n","w = torch.tensor([[0.5]])\n","b = torch.tensor([[-0.5]])\n","\n","# clone and normalize the data\n","x = X_train[:,0].clone().view(-1,1)\n","\n","# compute intermediate values\n","z = linear_regression(x, w, b)\n","y_pred = sigmoid(z)\n","\n","# compute gradients (v1)\n","dw1, db1 = compute_gradients(x, y_pred, y, w, b)\n","\n","# compute gradients (v2)\n","dw2, db2 = compute_gradients_v2(x, z, y_pred, y, w, b)"],"metadata":{"id":"s6Jkgv3m1VzF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Weight gradient : v1 ({dw1.item()}) and v2 ({dw2.item()})\")\n","print(f\"Bias gradient : v1 ({db1.item()}) and v2 ({db2.item()})\")"],"metadata":{"id":"xT2Q95E0BeUe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is the core idea behind the optimization of deep learning models: the backpropagation algorithm. Next lab you will see how is this implemented with PyTorch.\n","\n","In this lab we have implemented our own version of linear layer, sigmoid function, binary cross entropy, and even how to obtain the gradients for a log-linear classifier. However, as you will see in future labs, we do not need to do all of this with PyTorch, as it includes a set of built-in functions and modules, as well as an automatic differentiation package that will compute the gradients for us."],"metadata":{"id":"xotiQIf3x3TW"}}]}